import logging

import numpy as np
import sounddevice as sd
import onnxruntime as ort
import threading
import queue
import time
from collections import deque
from scipy import signal
from faster_whisper import WhisperModel
import keyboard
import argparse


logger = logging.getLogger(__name__)


class SpeechRecognizer:
    def __init__(self, model_size="small", language="ru", device="cpu", compute_type="int8"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Faster-Whisper –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏

        Args:
            model_size: —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ ("tiny", "base", "small", "medium", "large-v2", "large-v3")
            language: –æ—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫ ("ru", "en", "auto")
            device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ("cpu", "cuda")
            compute_type: —Ç–∏–ø –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ("int8", "int16", "float16", "float32")
        """
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Faster-Whisper: {model_size}")
        self.model = WhisperModel(
            model_size,
            device=device,
            compute_type=compute_type,
            download_root="models"
        )
        self.language = language
        self.sample_rate = 16000

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –¥–≤—É—è–∑—ã—á–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        if language == "ru":
            # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Å –∞–Ω–≥–ª–∏—Ü–∏–∑–º–∞–º–∏
            self.transcribe_options = {
                "language": "ru",
                "task": "transcribe",
                "temperature": 0.0,
                "best_of": 5,
                "beam_size": 5,
                "patience": 1,
                "length_penalty": 1,
                "suppress_tokens": [-1],
                # "multilingual": True,
                "initial_prompt": "–≠—Ç–æ —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç —Å –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏. Deployment –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ merge –≤ master –≤–µ—Ç–∫—É."
            }
        else:
            # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
            self.transcribe_options = {
                "task": "transcribe",
                "temperature": 0.0,
                "best_of": 3,
                "beam_size": 5
            }

        print("–ú–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def transcribe_audio(self, audio_data):
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–∑ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö

        Args:
            audio_data: numpy array —Å –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–º–∏ (16kHz, float32)

        Returns:
            tuple: (text, language, confidence)
        """
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞—É–¥–∏–æ
            if np.max(np.abs(audio_data)) > 0:
                audio_data = audio_data / np.max(np.abs(audio_data)) * 0.95

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (0.3 —Å–µ–∫—É–Ω–¥—ã)
            min_length = int(0.3 * self.sample_rate)
            if len(audio_data) < min_length:
                return "", "unknown", 0.0

            # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å
            segments, info = self.model.transcribe(
                audio_data,
                **self.transcribe_options
            )

            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            text_parts = []
            for segment in segments:
                if segment.text.strip():
                    text_parts.append(segment.text.strip())

            full_text = " ".join(text_parts).strip()

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —è–∑—ã–∫–µ
            detected_language = info.language if hasattr(info, 'language') else "unknown"
            confidence = info.language_probability if hasattr(info, 'language_probability') else 0.0

            return full_text, detected_language, confidence

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")
            return "", "unknown", 0.0


class RealTimeASR:
    def __init__(self,
                 target_sample_rate=16000,
                 block_size=1024,
                 whisper_model="small",
                 language="ru",
                 hotkey="ctrl+space"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏

        Args:
            target_sample_rate: —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
            block_size: —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –∞—É–¥–∏–æ
            whisper_model: —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ Whisper
            language: —è–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
            hotkey: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∫–ª–∞–≤–∏—à –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        """
        self.target_sample_rate = target_sample_rate
        self.block_size = block_size
        self.audio_queue = queue.Queue()
        self.running = False
        self.hotkey = hotkey

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—É—é —á–∞—Å—Ç–æ—Ç—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        self.device_sample_rate = self.find_supported_sample_rate()
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: {self.device_sample_rate} Hz")
        print(f"–ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç: {self.target_sample_rate} Hz")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —Ä–µ—á–∏
        self.speech_recognizer = SpeechRecognizer(
            model_size=whisper_model,
            language=language
        )

        # –ë—É—Ñ–µ—Ä –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.audio_buffer = deque()

        # –ë—É—Ñ–µ—Ä –¥–ª—è –∑–∞–ø–∏—Å–∏ —Ä–µ—á–µ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        self.speech_buffer = []
        self.recording_speech = False

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        self.manual_recording = False
        self.manual_start_time = None

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–µ—Å—ç–º–ø–ª–∏–Ω–≥–∞
        if self.device_sample_rate != self.target_sample_rate:
            self.need_resample = True
            self.resample_ratio = self.target_sample_rate / self.device_sample_rate
            print(f"–ë—É–¥–µ–º —Ä–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å —Å {self.device_sample_rate} Hz –¥–æ {self.target_sample_rate} Hz")
        else:
            self.need_resample = False

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        self.min_speech_length = int(0.3 * target_sample_rate)  # –º–∏–Ω–∏–º—É–º 0.3 —Å–µ–∫—É–Ω–¥—ã
        self.max_speech_length = int(60 * target_sample_rate)  # –º–∞–∫—Å–∏–º—É–º 60 —Å–µ–∫—É–Ω–¥

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–æ—Ä—è—á–∏—Ö –∫–ª–∞–≤–∏—à –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        self.setup_hotkeys()

    def setup_hotkeys(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–æ—Ä—è—á–∏—Ö –∫–ª–∞–≤–∏—à –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        try:
            keyboard.on_press_key(self.hotkey, self.on_hotkey_press, suppress=False)
            keyboard.on_release_key(self.hotkey, self.on_hotkey_release, suppress=False)
            print(f"–ì–æ—Ä—è—á–∏–µ –∫–ª–∞–≤–∏—à–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã: {self.hotkey}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–æ—Ä—è—á–∏—Ö –∫–ª–∞–≤–∏—à: {e}")
            print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å –ø—Ä–∞–≤–∞–º–∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º")

    def on_hotkey_press(self, event):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∂–∞—Ç–∏—è –≥–æ—Ä—è—á–µ–π –∫–ª–∞–≤–∏—à–∏"""
        if not self.manual_recording:
            self.manual_recording = True
            self.manual_start_time = time.time()
            self.speech_buffer = []
            print(f"üé§ –†–£–ß–ù–ê–Ø –ó–ê–ü–ò–°–¨ –ù–ê–ß–ê–õ–ê–°–¨ (–Ω–∞–∂–∞—Ç–∞ {self.hotkey})")

    def on_hotkey_release(self, event):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–ø—É—Å–∫–∞–Ω–∏—è –≥–æ—Ä—è—á–µ–π –∫–ª–∞–≤–∏—à–∏"""
        if self.manual_recording:
            self.manual_recording = False
            duration = time.time() - self.manual_start_time if self.manual_start_time else 0
            print(f"üîá –†–£–ß–ù–ê–Ø –ó–ê–ü–ò–°–¨ –û–°–¢–ê–ù–û–í–õ–ï–ù–ê (–æ—Ç–ø—É—â–µ–Ω–∞ {self.hotkey}, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.1f}—Å)")

            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            if len(self.speech_buffer) > 0:
                recognition_thread = threading.Thread(
                    target=self.process_speech_segment,
                    daemon=True
                )
                recognition_thread.start()

    def find_supported_sample_rate(self):
        """–ù–∞—Ö–æ–¥–∏–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—É—é —á–∞—Å—Ç–æ—Ç—É –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        sample_rates = [16000, 44100, 48000, 22050, 8000]

        try:
            default_device = sd.default.device[0]
            device_info = sd.query_devices(default_device)
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device_info['name']}")
        except:
            print("–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            default_device = None

        for sr in sample_rates:
            try:
                sd.check_input_settings(device=default_device, channels=1,
                                        samplerate=sr, dtype=np.float32)
                return sr
            except:
                continue

        try:
            device_info = sd.query_devices(default_device)
            return int(device_info['default_samplerate'])
        except:

            return 44100

    def resample_audio(self, audio_data):
        """–†–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ –¥–æ —Ü–µ–ª–µ–≤–æ–π —á–∞—Å—Ç–æ—Ç—ã"""
        if not self.need_resample:
            return audio_data

        original_length = len(audio_data)
        target_length = int(original_length * self.resample_ratio)

        if target_length == 0:
            return np.array([], dtype=np.float32)

        try:
            resampled = signal.resample(audio_data, target_length)
            return resampled.astype(np.float32)
        except:
            indices = np.linspace(0, original_length - 1, target_length)
            resampled = np.interp(indices, np.arange(original_length), audio_data)
            return resampled.astype(np.float32)

    def process_speech_segment(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–≥–æ —Ä–µ—á–µ–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        if len(self.speech_buffer) < self.min_speech_length:
            print("–°–µ–≥–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è")
            return

        print(f"–†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á–µ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç ({len(self.speech_buffer) / self.target_sample_rate:.1f}s)...")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array
        speech_array = np.array(self.speech_buffer, dtype=np.float32)

        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å
        text, language, confidence = self.speech_recognizer.transcribe_audio(speech_array)

        if text:
            print(f"\n{'=' * 60}")
            print(f"üéØ –†–ê–°–ü–û–ó–ù–ê–ù–û [{language.upper()}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}]:")
            print(f"   {text}")
            print(f"{'=' * 60}\n")
        else:
            print("–†–µ—á—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —à—É–º\n")

    def process_audio(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        print("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∞—É–¥–∏–æ...")

        while self.running:
            try:
                audio_chunk = self.audio_queue.get(timeout=1.0)

                if self.need_resample:
                    audio_chunk = self.resample_audio(audio_chunk)

                self.audio_buffer.extend(audio_chunk)

                while len(self.audio_buffer) >= 512:
                    chunk_data = np.array([self.audio_buffer.popleft() for _ in range(512)])


                    if self.manual_recording:
                        self.speech_buffer.extend(chunk_data)
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                        if len(self.speech_buffer) > self.max_speech_length:
                            self.speech_buffer = self.speech_buffer[-self.max_speech_length:]

                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–ø–∏—Å–∏
                        duration = len(self.speech_buffer) / self.target_sample_rate
                        print(f"üéôÔ∏è  –ó–ê–ü–ò–°–¨... {duration:.1f}s (—É–¥–µ—Ä–∂–∏–≤–∞–π—Ç–µ {self.hotkey})", end="\r")

            except queue.Empty:
                continue
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")

    def audio_callback(self, indata, frames, time, status):
        """Callback –¥–ª—è sounddevice"""
        if status:
            print(f"–ê—É–¥–∏–æ —Å—Ç–∞—Ç—É—Å: {status}")

        if len(indata.shape) > 1:
            audio_data = indata[:, 0]
        else:
            audio_data = indata.flatten()

        self.audio_queue.put(audio_data.copy())

    def start(self):
        """–ó–∞–ø—É—Å–∫ –∑–∞–ø–∏—Å–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.running = True

        processing_thread = threading.Thread(target=self.process_audio)
        processing_thread.daemon = True
        processing_thread.start()

        print(f"–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–ø–∏—Å—å —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ (—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device_sample_rate} Hz ‚Üí –º–æ–¥–µ–ª—å: {self.target_sample_rate} Hz)")
        print("–ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n")


        print(f"üìã –ò–ù–°–¢–†–£–ö–¶–ò–Ø:")
        print(f"   ‚Ä¢ –ù–∞–∂–º–∏—Ç–µ –∏ —É–¥–µ—Ä–∂–∏–≤–∞–π—Ç–µ {self.hotkey} –≤–æ –≤—Ä–µ–º—è –≥–æ–≤–æ—Ä–µ–Ω–∏—è")
        print(f"   ‚Ä¢ –û—Ç–ø—É—Å—Ç–∏—Ç–µ {self.hotkey} —á—Ç–æ–±—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ")
        print(f"   ‚Ä¢ –†–µ—á—å –±—É–¥–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ –æ—Ç–ø—É—Å–∫–∞–Ω–∏—è –∫–ª–∞–≤–∏—à–∏\n")


        try:
            with sd.InputStream(
                    channels=1,
                    samplerate=self.device_sample_rate,
                    blocksize=self.block_size,
                    callback=self.audio_callback,
                    dtype=np.float32
            ):
                while self.running:
                    time.sleep(0.1)

        except KeyboardInterrupt:
            print("\n–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å...")
        finally:
            self.stop()

    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–ø–∏—Å–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.running = False
        try:
            keyboard.unhook_all()
        except:
            pass
        print("–ó–∞–ø–∏—Å—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(
        description="Real-time Voice Activity Detection + Speech Recognition",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python script.py                                    # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º
  python script.py --mode manual                      # —Ä—É—á–Ω–æ–π —Ä–µ–∂–∏–º (Ctrl+Space)
  python script.py --mode manual --hotkey "ctrl+r"    # —Ä—É—á–Ω–æ–π —Ä–µ–∂–∏–º (Ctrl+R)
  python script.py --model medium --language en       # –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Å –º–æ–¥–µ–ª—å—é medium
        """
    )


    parser.add_argument(
        "--hotkey",
        default="ctrl",
        help="–ö–æ–º–±–∏–Ω–∞—Ü–∏—è –∫–ª–∞–≤–∏—à –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ctrl)"
    )

    parser.add_argument(
        "--model",
        choices=["tiny", "base", "small", "medium", "large-v2", "large-v3"],
        default="medium",
        help="–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ Whisper (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: small)"
    )

    parser.add_argument(
        "--language",
        default="ru",
        help="–Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ru)"
    )

    parser.add_argument(
        "--device",
        choices=["cpu", "cuda"],
        default="cpu",
        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: cpu)"
    )

    args = parser.parse_args()

    print("=== Real-time Voice Activity Detection + Speech Recognition ===")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ –∞—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:")
    devices = sd.query_devices()
    for i, device in enumerate(devices):
        if device['max_input_channels'] > 0:
            print(f"  {i}: {device['name']} (–≤—Ö–æ–¥—ã: {device['max_input_channels']})")

    print(f"\n{'=' * 70}")
    print(f"–ù–ê–°–¢–†–û–ô–ö–ò:")

    print(f"  –ì–æ—Ä—è—á–∏–µ –∫–ª–∞–≤–∏—à–∏:     {args.hotkey}")
    print(f"  –ú–æ–¥–µ–ª—å Whisper:      {args.model}")
    print(f"  –Ø–∑—ã–∫:                {args.language}")
    print(f"  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:          {args.device}")
    print(f"{'=' * 70}\n")

    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º—É
    try:
        vad_asr = RealTimeASR(
            target_sample_rate=16000,
            block_size=1024,
            whisper_model=args.model,
            language=args.language,
            hotkey=args.hotkey
        )
        vad_asr.start()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        if "keyboard" in str(e):
            print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
            print("1. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å –ø—Ä–∞–≤–∞–º–∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞")
            print("2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º: python script.py --mode auto")


if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:
    # pip install faster-whisper onnxruntime sounddevice numpy scipy keyboard
    #
    # –î–ª—è CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
    # pip install faster-whisper[gpu]

    main()